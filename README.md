
<p align="center" width="100%">
<img src="assets/logo4.png" alt="Guanaco" style="width: 90%; min-width: 300px; display: block; margin: auto;">
</p>

# Guanaco: Seeding, Tuning, Generating, and the problem of critical intervention in Large Language Models

The third animal that follows the pair llama and alpaca is guanaco.

This is the code repository for the 'Guanaco' lesson plan contained in *______________________________*, published by *________________*. Follow along with the lesson via the journal: *link TBC*.

Note: Guanaco is a hypothetical AI model for research and educational purposes.

## Overview

In February 2023 Meta released LLaMA, a set of large language models trained on billions of parameters. In March 2023 Stanford University researchers released [Alpaca](https://github.com/tatsu-lab/stanford_alpaca), a model of LLaMA fine-tuned on a synthetic dataset of 52,000 instruction parameters which had been generated by ChatGPT3 from 175 human-written instructions. Both the 175 human instructions and the 52 thousand generated parameters are publicly available on the [research team’s github](https://github.com/tatsu-lab/stanford_alpaca). The Stanford team’s raison d’être for the project was to provide a way of encouraging researchers to develop models which would overcome known ‘deficiencies’ in generative AI, especially the tendency to “generate false information, propagate social stereotypes, and produce toxic language”. 

Days later however, the live model was taken down due to ‘safety issues’ — despite its enlightened origins, the model displayed a noted tendency towards ‘hallucinating’ falsehoods and the production of ‘[offensive text](https://www.theregister.com/2023/03/21/stanford_ai_alpaca_taken_offline/)’.

The question is — why then did Alpaca fail, and what is it to ‘fine-tune’ an AI model? Thao Phan and Fabian Offert have criticised some recent efforts to ‘debias’ generative AI models as superficial, as failing especially to confront “the consistent reproduction of whiteness as a latent feature of dominant…culture” ([2022](https://arxiv.org/abs/2211.06323)). To what extent can a model be ‘tuned’ beyond or outside or against its foundational parameters and weightings? 

The lesson proposed [here]() takes two steps. In the first the classroom will be invited to compare Alpaca’s 175 human instructions with the 52,000 GPT3-generated instruction parameters, and in the second the classroom will generate a new AI model within the camelid family, Guanaco.

Students will be asked to reflect upon the relation between seed, generation, and hallucination. To what degree is political intervention possible in large language models? To what degree can experiments in tuning make present and detourn models’ foundations, and what consequences do such experiments have in a chaotic age of increasingly accessible and normalised access to generative AI?

## Lesson Plan

Note: The full lesson plan is featured in *_________________*: *link TBC*

### Step one:

Compare the [175 seed tasks](./alpaca_seed_tasks.jsonl) with the [52,000 GPT3-generated instruction parameters](./alpaca_generated_data.json).

#### Key texts:
- Stanford Alpaca's release [blog post](https://crfm.stanford.edu/2023/03/13/alpaca.html).
- The [Self-Instruct paper](https://arxiv.org/abs/2212.10560) by Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi. 

### Step two: creating Guanaco

As a class, manually write your own [175 original seed tasks](./guanaco_seed_tasks.jsonl).

#### Key reading:
- [A Sign That Spells](https://arxiv.org/abs/2211.06323) by Fabian Offert and Thao Phan.

To generate a synthetic dataset from your original seed tasks please follow the steps outlined below.

#### Data Generation Process

This data generation pipeline was built by the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) team, on top of the framework designed for [self-instruct](https://github.com/yizhongw/self-instruct).

Ensure you have Python installed on your computer before starting.

<details>
<summary> 1. Downloading the code repository from GitHub</summary>

- Download the repository: On our [GitHub page](https://github.com/queercomputer/Guanaco/), look for the green button labeled "Code" near the top of the page. Click on it and then select "Download ZIP" from the dropdown menu.

- Extract the ZIP file: Once the download is complete, locate the ZIP file on your computer and extract it.

- Ensure your classroom's version of the Guanaco seed-set is saved inside this folder as 'guanaco_seed_tasks.jsonl'.

</details>

<details>
<summary> 2. Installing Python modules</summary>

- Open Command Line Interface:

    - On Windows, press Windows Key + R, type cmd, and press Enter.

    - On Mac, open the Terminal application from your Applications/Utilities folder.

- Navigate to the project directory: Use the cd command followed by the path to the extracted folder to change your current directory to the location of the downloaded code. For example: 

```
cd Downloads/Guanaco-main
```

- Install the required Python modules. Type:

```
pip install -r requirements.txt
```
- Hit Enter. This command reads the requirements.txt file included in your project folder and installs all the necessary Python modules listed there.

</details>

<details>
<summary> 3. Setting up an OpenAI API Key</summary>

- Create an OpenAI account: Go to the [OpenAI website](https://platform.openai.com/api-keys) and sign up for an account if you don’t already have one.

- Generate an API key: Once logged in, navigate to the [API section](https://platform.openai.com/api-keys) and follow the instructions to generate a new API key. Keep this key confidential.

- Configure your project: Open the project folder and locate the configuration file — 'utils.py'. Open it with a text editor and replace the placeholder text with your OpenAI API key. 

```
OPENAI_API_KEY = "your_key_here"
```

</details>

<details>
<summary> 4. Running the Python script</summary>

- Return to your Command Line Interface.

- Navigate back to your project folder (if not already there): Use the cd command to change your current directory to the location of the downloaded code. For example: 

```
cd Downloads/Guanaco-main
```

- Run the script: Type: 

```
python generate_instruction.py
```

- Hit Enter. The script will execute and perform the tasks as programmed.

</details>

<details>
<summary> 5. Your Guanaco dataset</summary>

- The synthetic dataset will be outputted as ‘guanaco_generated_data.json’ within the same folder.

- In our testing, generating a thousand additional tasks took approximately 1 hour and 20 minutes, incurring a cost of 50 cents. These figures can vary based on several factors, but it is relatively cheap.

- While the full dataset generation may take some time, the 'guanaco_generated_data.json' file will begin to populate with tasks almost immediately. This provides an opportunity for real-time observation and discussion in class. We recommend reflecting on the finished Guanaco synthetic dataset online or part of a subsequent lesson.

</details>

### Authors

- [Joel Humphries](https://joelhumphries.glitch.me)
- [Dr Christopher O’Neill](https://www.admscentre.org.au/christopher-oneill/)


### Acknowledgements

We'd like to acknowledge the work of:

- the Stanford Alpaca team for their work on the Alpaca model and [code repository](https://github.com/tatsu-lab/stanford_alpaca): Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang and Tatsunori B. Hashimoto.

- the University of Washington Natural Language Processing lab and everyone who worked on the [Self-Instruct paper](https://arxiv.org/abs/2212.10560) and the self-intruct [repository](https://github.com/yizhongw/self-instruct): Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi and Hannaneh Hajishirzi.
